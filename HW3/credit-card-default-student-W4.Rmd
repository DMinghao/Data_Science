---
title: "Credit Card Default"
output:
  word_document:
    toc: yes
  html_notebook:
    number_sections: yes
    toc: yes
---

# Intro
As always, let's start with clearing the workspace and load required packages. 
For this exercise, we will use the dataset, *Default* dataset, which is available from *ISLR* library. 
ISLR packages provide several datasets. The description of datasets are available here: http://cran.r-project.org/web/packages/ISLR/ISLR.pdf

```{r}
rm(list = ls())     # clear the workspace 
library(ISLR)       # load ISLR data package
library(tidyverse)
library(ggplot2)
```


# Data Preparation
Examine the dataset. It contains four variables, `default`, `student`,`balance`, and `income`.  
```{r}
Default<-as_tibble(Default)
Default
glimpse(Default)       

head(Default) # show the first six rows
tail(Default) # show the last six rows
names(Default)      # variable names
nrow(Default)       # the number of rows
ncol(Default)       # the number of columns
summary(Default)    # basic summary statistics of the variables in Default dataset (default, student, balance, income)
```

```{r}
# frequency table 
summary(Default$default)   # summary of default variable
table(Default$default)     # contingency table: frequency of each case (yes/no) in default variable
table(Default$student)     # contingency table: frequency of each case (yes/no) in student variable
table(Default$default, Default$student)  # cross-tabulation
```

# Data Visualization
## Bar chart
```{r}
Default %>%
  ggplot(aes(x=default,fill=default)) +
  geom_bar()

Default %>%
  ggplot(aes(x=student,fill=student)) +
  geom_bar()
```

## Histograms
```{r}
Default %>%
  ggplot(aes(x=income)) +
  geom_histogram(binwidth=1000, colour="black",fill="white")

Default %>%
  ggplot(aes(x=income,fill=student)) +
  geom_histogram(binwidth=1000,alpha=.5,position="identity")

Default %>%
  ggplot(aes(x=income,fill=default)) +
  geom_histogram(binwidth=1000,alpha=.5,position="identity")
```

## Boxplots
```{r}
ggplot(Default,aes(x=default,y=balance,fill=default))+geom_boxplot()
ggplot(Default,aes(x=default,y=income,fill=default))+geom_boxplot()
```

## Scatter plots
```{r}
Default %>%
  ggplot(aes(x=balance,y=income,color=default)) +
  geom_point(shape=1)
```

How do these visualizations help us predict default? 

# Week 3: Modeling - Tree-based classification model 
## Classification Trees
We need *rpart* package for Classification Trees. More on rpart : http://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf

`rpart.plot` is a package for visualizing classification tree models. 
```{r}
library(rpart)
library(rpart.plot)
```

Let's build a model. 
```{r}
ct_model<-rpart(default~student+balance+income,           # model formula
                data=Default,                             # dataset
                method="class",                           # "class" indicates a classification tree model 
                control=rpart.control(cp=0,maxdepth=4))   # tree control parameters. 
```
You can use `rpart` function to build a regression tree, but we will not do it in this class. `method="class"` indicates that we want to build a classification tree model. 

By changing the parameter values for `control`, you can change how the model is trained (and shaped). Try changing the values and see how the model changes. For more information, `?rpart.control`. These are some examples. 

* minsplit: minimum number of data points required to attempt a split
* cp: complexity parameter
* maxdepth: depth of a classification tree 


Which value should we choose? We will discuss it later with model evaluation (Week 5). 

Next, let's visualize the tree model. 
```{r}
rpart.plot(ct_model)   # tree plot
```

Here, I tried to visualize the classification tree model results on the scatterplot. It could be done, because the model used the two numeric variables. 
```{r}
Default %>%
  ggplot(aes(x=balance,y=income,color=default)) +
  geom_point(shape=1)+
  geom_vline(xintercept=1800.002,linetype="dashed")+
  geom_vline(xintercept=1971.915,linetype="dashed")+
  geom_hline(yintercept=27401.2,linetype="dashed")+
  annotate("rect",xmin=1800.002, xmax=1971.915, ymin=0, ymax=27401.2,fill="red",alpha=0.2)+
  annotate("rect",xmin=1971.915, xmax=Inf, ymin=0, ymax=Inf,fill="blue",alpha=0.2)+
  annotate("rect",xmin=0, xmax=1800.002, ymin=0, ymax=Inf,fill="red",alpha=0.2)+
  annotate("rect",xmin=1800.002, xmax=1971.915, ymin=27401.2, ymax=Inf,fill="blue",alpha=0.2)
```

```{r}
#print(ct_model)        # model results 
```


Get the predicted value - class membership (yes or no) --> using a cut-off of 50%. 
```{r}
ct_pred_class<-predict(ct_model,type="class") # class membership (yes or no) 
head(ct_pred_class)

ct_pred<-predict(ct_model)  # get the predicted values - class probabilities (default)
head(ct_pred)
```

Let's create a new column in Default: save the predicted probability of default (yes) from the second column of dt_pred.
```{r}
Default$ct_pred_prob<-ct_pred[,2]   
```

Alternatively, you can specify a certain cut-off value to assign class membership. You can set the cut-off at 30%, 50%, 80%, or whatever you want. 
```{r}
Default$ct_pred_class<-ifelse(Default$ct_pred_prob>0.5,"Yes","No")   
```

                                                                    
```{r}
head(Default)
Default[253,]     # get the information of 253th customer 

# show the customers whose predicted probability is greater than 70%
Default%>%
  filter(ct_pred_prob>0.7) 

# sort customers by probability of default in descending order
Default%>%
  arrange(desc(ct_pred_prob))
```

## Random Forest
```{r}
set.seed(1)
#install.packages("randomForest")
library(randomForest)
rf_model<-randomForest(default~income+balance+student,              # model formula
                       data=Default,ntree=500, cutoff=c(0.5,0.5))
```

```{r}
#print(rf_model)
head(rf_model$votes)       # indicates the % of trees that voted for each class
head(rf_model$predicted)   # the class favored by more trees (i.e. majority vote wins) 
```

```{r}
varImpPlot(rf_model)  # importance of variables 
```
```{r}
head(rf_model$vote)

Default$rf_vote<-predict(rf_model,type="prob")[,2]
head(Default)
```
# Week 4: Modeling - Linear classifier 
## Support Vector Machine (SVM)
```{r}
library(e1071)
model_svm<-svm(formula= default ~ balance+income+student, # model formula 
               data=Default,                   # dataset
               kernel="linear",  # this is the form of the decision boundary. Let's start with a linear kernel. 
               cost=0.1)        # there are paremeters that are used to tune the model 
model_svm
```
The model may not converge, and it is not uncommon. Also, note that it is not an error. It is less desire, but it provides classification results. To improve performance, you may try different cost parameters, or you may even try other kernel functions, other than "linear". Other option is normalizing data. But we will move on with this result. 

Conceptually, you may interpret decision values as the distance between the observation and the decision boundary. The positive fitted value indicate one class, and negative value indicates the other class. 

```{r}
dv<-data.frame(model_svm$decision.values)

ggplot(dv,aes(x=No.Yes)) +
  geom_histogram(colour="black",fill="white")
```


```{r}
head(model_svm$fitted) 
table(model_svm$fitted)

predicted_svm<-predict(model_svm,Default,decision.values = TRUE)
head(attr(predicted_svm, "decision.values"))
```

```{r}
Default

Default$svm_pred_class <- predict(model_svm, Default) 
Default$svm_dv<-c(attr(predicted_svm, "decision.values"))

Default
```

## Logistic Regression
```{r}
logit_model<-glm(default~student+balance+income,  # generalized linear models
                 family="binomial",               # specifying error distribution
                 data=Default)                    # dataset
summary(logit_model)
```

### Use of the model to predict
```{r}
Default$log_odd<-predict(logit_model)                         # get predicted log odds (default)
Default$logit_pred_prob<-predict(logit_model,type="response") # get predicted probabilities
glimpse(Default)
Default%>%
  select("default","student","log_odd","logit_pred_prob")
```

With the predicted probabilities, you can sort customers by the predicted probability of default in descending order. 
```{r}
Default%>%
  arrange(desc(logit_pred_prob))%>%
  select(default, student, balance, income, ct_pred_prob, ct_pred_class,logit_pred_prob)
```

And use a different cut-off for class prediction using `ifelse()`. 
```{r}
Default$logit_pred_class<-ifelse(Default$logit_pred_prob>0.5,"Yes","No")
Default
```

