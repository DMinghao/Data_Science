---
title: "Credit Card Default"
output:
  word_document:
    toc: yes
  html_notebook:
    number_sections: yes
    toc: yes
---

# Intro
As always, let's start with clearing the workspace and load required packages. 
For this exercise, we will use the dataset, *Default* dataset, which is available from *ISLR* library. 
ISLR packages provide several datasets. The description of datasets are available here: http://cran.r-project.org/web/packages/ISLR/ISLR.pdf

```{r}
rm(list = ls())     # clear the workspace 
library(ISLR)       # load ISLR data package
library(tidyverse)
library(ggplot2)
head(Default)
```


# Data Preparation
Let's first examine the dataset. It contains four variables, `default`, `student`,`balance`, and `income`.  
```{r}
Default<-as_tibble(Default)
Default
glimpse(Default)       
```

```{r}
# frequency table 
summary(Default$default)   # summary of default variable
table(Default$default)     # contingency table: frequency of each case (yes/no) in default variable

table(Default$student)
# contingency table: frequency of each case (yes/no) in student variable
table(Default$default, Default$student)  # cross-tabulation
```

# Data Visualization
## Bar chart
```{r}
Default %>%
  ggplot(aes(x=default,fill=default)) +
  geom_bar()

Default %>%
  ggplot(aes(x=student,fill=student)) +
  geom_bar()
```

## Histograms
```{r}
Default %>%
  ggplot(aes(x=income)) +
  geom_histogram(binwidth=1000, colour="black",fill="white")

Default %>%
  ggplot(aes(x=income,fill=student)) +
  geom_histogram(binwidth=1000,alpha=.5,position="identity")

Default %>%
  ggplot(aes(x=income,fill=default)) +
  geom_histogram(binwidth=1000,alpha=.5,position="identity")
```

## Boxplots
```{r}
ggplot(Default,aes(x=default,y=balance,fill=default))+geom_boxplot()
ggplot(Default,aes(x=default,y=income,fill=default))+geom_boxplot()
```

## Scatter plots
```{r}
Default %>%
  ggplot(aes(x=balance,y=income,color=default)) +
  geom_point(shape=1)
```

How do these visualizations help us predict default? 

# Week 3: Modeling - Tree-based classification model 
## Classification Trees
We need *rpart* package for Classification Trees. More on rpart : http://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf

`rpart.plot` is a package for visualizing classification tree models. 
```{r}
library(rpart)
library(rpart.plot)
```

Let's build a model. 
```{r}
ct_model<-rpart(default ~ student+balance+income,           # model formula
                data=Default,                               # dataset
                method="class",                             # "class" indicates a classification tree model 
                control=rpart.control(cp=0.03,maxdepth=4))  # tree control parameters. 
```
You can use `rpart` function to build a regression tree, but we will not do it in this class. `method="class"` indicates that we want to build a classification tree model. 

By changing the parameter values for `control`, you can change how the model is trained (and shaped). Try changing the values and see how the model changes. For more information, `?rpart.control`. These are some examples. 

* minsplit: minimum number of data points required to attempt a split
* cp: complexity parameter
* maxdepth: depth of a classification tree 

Which value should we choose? We will discuss it later with model evaluation (Week 5). 

Next, let's visualize the tree model. 
```{r}
rpart.plot(ct_model)   # tree plot
```

Here, I tried to visualize the classification tree model results on a scatterplot. It could be done because the model used the two numeric variables. 
```{r}
Default %>%
  ggplot(aes(x=balance,y=income,color=default)) +
  geom_point(shape=1)+
  geom_vline(xintercept=1800.002,linetype="dashed")+
  geom_vline(xintercept=1971.915,linetype="dashed")+
  geom_hline(yintercept=27401.2,linetype="dashed")+
  annotate("rect",xmin=1800.002, xmax=1971.915, ymin=0, ymax=27401.2,fill="red",alpha=0.2)+
  annotate("rect",xmin=1971.915, xmax=Inf, ymin=0, ymax=Inf,fill="blue",alpha=0.2)+
  annotate("rect",xmin=0, xmax=1800.002, ymin=0, ymax=Inf,fill="red",alpha=0.2)+
  annotate("rect",xmin=1800.002, xmax=1971.915, ymin=27401.2, ymax=Inf,fill="blue",alpha=0.2)
```

```{r}
#print(ct_model)        # model result: ignore this and use our tree plot to understand the model
```

Get the predicted results - class membership (yes or no) --> using a cut-off of 50%. 
```{r}
ct_pred_class<-predict(ct_model,type="class")   #predict class membership (yes or no) 
head(ct_pred_class)

ct_pred<-predict(ct_model)      #get the predicted values - class probabilities (default)
head(ct_pred)
head(ct_pred[,2])
```

Let's create a new column in Default: save the predicted probability of default (yes) from the *second* column of dt_pred.
```{r}
head(Default)
Default$ct_pred_prob<-ct_pred[,2]  
head(Default)
```

Alternatively, you can specify a certain cut-off value to assign class membership. You can set the cut-off at 30%, 50%, 80%, or whatever you want. 
```{r}
Default$ct_pred_class<-ifelse(Default$ct_pred_prob>0.5,"Yes","No")   
head(Default)
```

                                                                    
```{r}
head(Default)
Default[253,]     # get the information of 253th customer 

# show the customers whose predicted probability is greater than 70%
Default%>%
  filter(ct_pred_prob>0.7) 

# sort customers by probability of default in descending order
Default%>%
  arrange(desc(ct_pred_prob))
```

## Random Forest
```{r}
library(randomForest)

set.seed(1)       
rf_model<-randomForest(default~income+balance+student,              # model formula
                       data=Default,ntree=500, cutoff=c(0.5,0.5))
```

```{r}
head(rf_model$votes)       # indicates the % of trees that voted for each class
head(rf_model$predicted)   # the class favored by more trees (i.e. majority vote wins) 
```

```{r}
varImpPlot(rf_model)  # importance of variables 
```

```{r}
head(rf_model$vote)

Default$rf_vote<-rf_model$votes[,2]
head(Default)
```
